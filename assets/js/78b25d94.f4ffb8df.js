"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[111],{2362:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>l,toc:()=>d});var r=i(5893),t=i(1151);const s={sidebar_position:11},a="Finetune",l={id:"API-Reference/finetune",title:"Finetune",description:"OpenAIFineTuneUtils",source:"@site/docs/API-Reference/finetune.md",sourceDirName:"API-Reference",slug:"/API-Reference/finetune",permalink:"/YiValApi/docs/API-Reference/finetune",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/API-Reference/finetune.md",tags:[],version:"current",sidebarPosition:11,frontMatter:{sidebar_position:11},sidebar:"tutorialSidebar",previous:{title:"Data Utils",permalink:"/YiValApi/docs/API-Reference/data-utils"},next:{title:"Variation Generator",permalink:"/YiValApi/docs/API-Reference/variation-generator"}},o={},d=[{value:"<code>OpenAIFineTuneUtils</code>",id:"openaifinetuneutils",level:2},{value:"Introduction",id:"introduction",level:3},{value:"Class Definition",id:"class-definition",level:3},{value:"Description",id:"description",level:4},{value:"Attributes",id:"attributes",level:4},{value:"Example",id:"example",level:3},{value:"<code>SFTTrainer</code>",id:"sfttrainer",level:2},{value:"Introduction",id:"introduction-1",level:3},{value:"<code>DataSetConfig</code>",id:"datasetconfig",level:3},{value:"Description",id:"description-1",level:4},{value:"Attributes",id:"attributes-1",level:4},{value:"<code>TrainArguments</code>",id:"trainarguments",level:3},{value:"Description",id:"description-2",level:4},{value:"Attributes",id:"attributes-2",level:4},{value:"<code>BnbConfig</code>",id:"bnbconfig",level:3},{value:"Description",id:"description-3",level:4},{value:"Attributes",id:"attributes-3",level:4},{value:"<code>LoRAConfig</code>",id:"loraconfig",level:3},{value:"Description",id:"description-4",level:4},{value:"Attributes",id:"attributes-4",level:4},{value:"Example",id:"example-1",level:3},{value:"Dataset load and finetune model in yival config",id:"dataset-load-and-finetune-model-in-yival-config",level:4},{value:"<code>BackUpSFTTrainer</code>",id:"backupsfttrainer",level:2},{value:"Introduction",id:"introduction-2",level:3},{value:"Class Definition",id:"class-definition-1",level:3},{value:"Description",id:"description-5",level:4},{value:"Attributes",id:"attributes-5",level:4},{value:"Example",id:"example-2",level:3}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"finetune",children:"Finetune"}),"\n",(0,r.jsx)(n.h2,{id:"openaifinetuneutils",children:(0,r.jsx)(n.code,{children:"OpenAIFineTuneUtils"})}),"\n",(0,r.jsx)(n.h3,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsx)(n.p,{children:"This module is designed to fine-tune OpenAI's GPT model using a specified dataset and conditions. It provides a utility for extracting relevant results from an experiment, transforming this data into the desired format, and then leveraging OpenAI's API to perform fine-tuning."}),"\n",(0,r.jsx)(n.p,{children:"It takes ExperimentResult dump (pkl), and filtered conditions to extract the result."}),"\n",(0,r.jsx)(n.h3,{id:"class-definition",children:"Class Definition"}),"\n",(0,r.jsx)(n.h4,{id:"description",children:"Description"}),"\n",(0,r.jsx)(n.h4,{id:"attributes",children:"Attributes"}),"\n",(0,r.jsx)(n.h3,{id:"example",children:"Example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-Python",children:"def main():\r\n    finetune(\r\n        'test_demo_results.pkl',\r\n        \"name == openai_prompt_based_evaluator AND result >= 0 AND display_name == clarity\",\r\n    )\n"})}),"\n",(0,r.jsx)(n.h2,{id:"sfttrainer",children:(0,r.jsx)(n.code,{children:"SFTTrainer"})}),"\n",(0,r.jsx)(n.h3,{id:"introduction-1",children:"Introduction"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"SFTTrainer"})," class is designed to finetune a pre-trainer model on a specific task. It's a part of the yival framework that allows for easy and efficient fine-tuning of models. The class is built on top of the Hugging Face Transformers library and provides a high-level, easy-to-use API for fine-tuning."]}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"SFTTrainer"})," class supports fine-tuning with different configurations, including enabling bits and bytes for quantization and LoRA for low-rank adaptation. It also supports different training arguments such as batchsize\u3001learning_rate and number of epochs."]}),"\n",(0,r.jsx)(n.p,{children:"We recommend two ways to use Yival's SFTTrainer."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Use various dataset generators built into Yival (including huggingface, openai_generator, etc.) for data upload or generation, and then finetune the model"}),"\n",(0,r.jsx)(n.li,{children:"Provide a custom_func, use advanced models like GPT-4 for data generation, and customize the selection criteria. The model is then finetuned based on the selected data."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"datasetconfig",children:(0,r.jsx)(n.code,{children:"DataSetConfig"})}),"\n",(0,r.jsx)(n.h4,{id:"description-1",children:"Description"}),"\n",(0,r.jsx)(n.p,{children:"Configuration class for the training arguments. It specifies various parameters including per device train batch size, gradient accumulation steps, gradient checkpointing, max grad norm, num train epochs, learning rate, bf16, save total limit, logging steps, optim, lr scheduler type, warmup ratio, and log level."}),"\n",(0,r.jsx)(n.h4,{id:"attributes-1",children:"Attributes"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"per_device_train_batch_size(int)"})}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"specifies the batch size for training per device."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"gradient_accumulation_steps(int)"})}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"specifies the number of steps to accumulate gradients before updating."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"gradient_checkpointing(bool)"})}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"specifies whether to use gradient checkpointing to save memory."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"max_grad_norm(float)"})}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"specifies the maximum norm of the gradients."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"num_train_epochs(int)"})}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"specifies the number of training epochs."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"learning_rate(float)"})}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"specifies the learning rate."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"bf16(bool)"})}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"specifies whether to use bf16 precision for training."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"save_total_limit(int)"})}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"specifies the total number of checkpoints to save."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"logging_steps(int)"})}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"specifies the number of steps between logging."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"optim(str)"})}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"specifies the optimizer to use for training."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"lr_scheduler_type(str)"})}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"specifies the type of learning rate scheduler to use."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"warmup_ratio(float)"})}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"specifies the warmup ratio for the learning rate scheduler."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"log_level(str)"})}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"specifies the log level."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"trainarguments",children:(0,r.jsx)(n.code,{children:"TrainArguments"})}),"\n",(0,r.jsx)(n.h4,{id:"description-2",children:"Description"}),"\n",(0,r.jsx)(n.p,{children:"Configuration class for the dataset. It specifies various parameters including prompt key, completion key, formatting prompts format, and condition."}),"\n",(0,r.jsx)(n.h4,{id:"attributes-2",children:"Attributes"}),"\n",(0,r.jsx)(n.h3,{id:"bnbconfig",children:(0,r.jsx)(n.code,{children:"BnbConfig"})}),"\n",(0,r.jsx)(n.h4,{id:"description-3",children:"Description"}),"\n",(0,r.jsx)(n.p,{children:"Configuration class for the bits and bytes. It specifies whether to load in 4 bit or 8 bit."}),"\n",(0,r.jsx)(n.h4,{id:"attributes-3",children:"Attributes"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"load_in_4_bit(bool)"})}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"specifies whether to load in 4 bit."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"load_in_8_bit(bool)"})}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"specifies whether to load in 8 bit."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"loraconfig",children:(0,r.jsx)(n.code,{children:"LoRAConfig"})}),"\n",(0,r.jsx)(n.h4,{id:"description-4",children:"Description"}),"\n",(0,r.jsx)(n.p,{children:"Configuration class for the LoRA. It specifies various parameters including r, lora alpha, bias, task type, lora dropout, and inference mode."}),"\n",(0,r.jsx)(n.h4,{id:"attributes-4",children:"Attributes"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"r(int)"})}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"specifies the rank for the LoRA."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"lora_alpha(int)"})}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"specifies the alpha for the LoRA."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"bias(str)"})}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"specifies the bias for the LoRA."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"task_type(str)"})}),":"]}),"\n",(0,r.jsx)(n.li,{children:"specifies the task type for the LoRA."}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"lora_dropout(float)"})}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"specifies the dropout for the LoRA."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"inference_mode(bool)"})}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"specifies whether to use inference mode for the LoRA."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"example-1",children:"Example"}),"\n",(0,r.jsx)(n.h4,{id:"dataset-load-and-finetune-model-in-yival-config",children:"Dataset load and finetune model in yival config"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-YAML",children:'description: Generated experiment config\r\ndataset:\r\n  data_generators:\r\n    openai_prompt_data_generator:\r\n      chunk_size: 100000\r\n      diversify: true\r\n      # model_name specify the llm model , e.g. a16z-infra/llama-2-13b-chat:9dff94b1bed5af738655d4a7cbcdcde2bd503aa85c94334fe1f42af7f3dd5ee3\r\n      model_name: gpt-4\r\n      prompt:\r\n          "Please provide a concrete and realistic test case as a dictionary for function invocation using the ** operator.\r\n          Only include parameters, excluding description and name.\r\n          Ensure it\'s succinct and well-structured.\r\n          **Only provide the dictionary.**"\r\n      input_function:\r\n        description:\r\n          The current function is to evaluate the English to Chinese translation ability of the large language model. You will play the role of a teacher, so please provide a coherent English sentence (teacher_quiz), and give the corresponding Chinese translation (teachaer_answer).\r\n        name: translation_english_to_chinese\r\n        parameters:\r\n          teacher_quiz: str\r\n          teacher_answer: str\r\n      expected_param_name: teacher_answer\r\n      number_of_examples: 5\r\n      output_path: english2chinese1.pkl\r\n  source_type: machine_generated\r\n\r\ntrainer:\r\n  name: sft_trainer\r\n  model_name: PY007/TinyLlama-1.1B-Chat-v0.3\r\n  output_path: output\r\n  dataset_config:\r\n    prompt_key: teacher_quiz\r\n  enable_lora: False\r\n  enable_bits_and_bytes: False\n'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Custom_func then customize the selection criteria. The model is then finetuned based on the selected data in yival config"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-YAML",children:'custom_function: demo.headline_generation_detail.headline_generation\r\ndescription: Generated experiment config\r\ndataset:\r\n  data_generators:\r\n    openai_prompt_data_generator:\r\n      chunk_size: 100000\r\n      diversify: true\r\n      # model_name specify the llm model , e.g. a16z-infra/llama-2-13b-chat:9dff94b1bed5af738655d4a7cbcdcde2bd503aa85c94334fe1f42af7f3dd5ee3\r\n      model_name: gpt-4\r\n      prompt:\r\n          "Please provide a concrete and realistic test case as a dictionary for function invocation using the ** operator.\r\n          Only include parameters, excluding description and name.\r\n          Ensure it\'s succinct and well-structured.\r\n          **Only provide the dictionary.**"\r\n      input_function:\r\n        description:\r\n          "Given a tech startup business named [tech_startup_business], specializing in [business], and target_peopleing [target_people], generate a corresponding landing page headline."\r\n        name: headline_generation_for_business\r\n        parameters:\r\n          tech_startup_business: str\r\n          business: str\r\n          target_people: str\r\n      number_of_examples: 3\r\n      output_path: null\r\n  source_type: machine_generated\r\n\r\nvariations:\r\n  - name: task\r\n    variations:\r\n      - instantiated_value: Generate landing page headline for {tech_startup_business}, company business is {business}, target_people is {target_people}\r\n        value: Generate landing page headline for {tech_startup_business}, company business is {business}, target_people is {target_people}\r\n        value_type: str\r\n        variation_id: null\r\n\r\nevaluators:\r\n  - evaluator_type: individual\r\n    metric_calculators:\r\n      - method: AVERAGE\r\n    name: openai_prompt_based_evaluator\r\n    display_name: clear\r\n    prompt: |-\r\n      You are assessing a submitted answer on a given task based on a criterion. Here is the data:\r\n      - Task: Given an tech startup business, generate one corresponding landing page headline\r\n      - Does the headline clearly communicate what the startup does or what problem it solves?\r\n        It should be immediately clear to anyone who reads the headline what the startup\'s purpose is.\r\n        A lack of clarity can lead to confusion and may discourage potential users or investors.\r\n      [Input]: {tech_startup_business}\r\n      [Result]: {raw_output}\r\n      Answer the question by selecting one of the following options:\r\n      A It fails to meet the criterion at all.\r\n      B It somewhat meets the criterion, but there is significant room for improvement.\r\n      C It meets the criterion to a satisfactory degree.\r\n      D It meets the criterion very well.\r\n      E It meets the criterion exceptionally well, with little to no room for improvement.\r\n    choices: ["A", "B", "C", "D", "E"]\r\n    # model_name specify the llm model , e.g. a16z-infra/llama-2-13b-chat:9dff94b1bed5af738655d4a7cbcdcde2bd503aa85c94334fe1f42af7f3dd5ee3\r\n    model_name: gpt-4\r\n    description: "evaluate the quality of the landing page headline"\r\n    scale_description: "0-4"\r\n    choice_scores:\r\n      A: 0\r\n      B: 1\r\n      C: 2\r\n      D: 3\r\n      E: 4\r\n\r\nselection_strategy:\r\n  ahp_selection:\r\n    criteria:\r\n      - "openai_prompt_based_evaluator: clear"\r\n      - average_token_usage\r\n      - average_latency\r\n    criteria_maximization:\r\n      "openai_prompt_based_evaluator: clear": true\r\n      average_latency: false\r\n      average_token_usage: false\r\n    criteria_weights:\r\n      "openai_prompt_based_evaluator: clear": 0.6\r\n      average_latency: 0.2\r\n      average_token_usage: 0.2\r\n    normalize_func: "z-score"\r\n\r\ntrainer:\r\n  name: sft_trainer\r\n  model_name: PY007/TinyLlama-1.1B-Chat-v0.3\r\n  output_path: output\r\n  dataset_config:\r\n    prompt_key: teacher_quiz\r\n    condition: "name == openai_prompt_based_evaluator AND result >= 0 AND display_name == clear"\r\n  enable_lora: False\r\n  enable_bits_and_bytes: False\n'})}),"\n",(0,r.jsx)(n.h2,{id:"backupsfttrainer",children:(0,r.jsx)(n.code,{children:"BackUpSFTTrainer"})}),"\n",(0,r.jsx)(n.h3,{id:"introduction-2",children:"Introduction"}),"\n",(0,r.jsx)(n.h3,{id:"class-definition-1",children:"Class Definition"}),"\n",(0,r.jsx)(n.h4,{id:"description-5",children:"Description"}),"\n",(0,r.jsx)(n.h4,{id:"attributes-5",children:"Attributes"}),"\n",(0,r.jsx)(n.h3,{id:"example-2",children:"Example"})]})}function h(e={}){const{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},1151:(e,n,i)=>{i.d(n,{Z:()=>l,a:()=>a});var r=i(7294);const t={},s=r.createContext(t);function a(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);