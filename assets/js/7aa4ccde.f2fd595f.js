"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[455],{9490:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>s,contentTitle:()=>r,default:()=>u,frontMatter:()=>l,metadata:()=>o,toc:()=>c});var a=i(5893),t=i(1151);const l={sidebar_position:4},r="Evaluation",o={id:"API-Reference/evaluation",title:"Evaluation",description:"BaseEvaluator",source:"@site/docs/API-Reference/evaluation.md",sourceDirName:"API-Reference",slug:"/API-Reference/evaluation",permalink:"/YiValApi/docs/API-Reference/evaluation",draft:!1,unlisted:!1,editUrl:"https://github.com/YiVal/YiVal/tree/master/website/docs/API-Reference/evaluation.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"Reader",permalink:"/YiValApi/docs/API-Reference/reader"},next:{title:"Wrapper",permalink:"/YiValApi/docs/API-Reference/wrapper"}},s={},c=[{value:"<code>BaseEvaluator</code>",id:"baseevaluator",level:2},{value:"Introduction",id:"introduction",level:3},{value:"Class Definition",id:"class-definition",level:3},{value:"Description",id:"description",level:4},{value:"Attributes",id:"attributes",level:4},{value:"Example",id:"example",level:3},{value:"Source Code",id:"source-code",level:3},{value:"<code>EvaluatorType</code>",id:"evaluatortype",level:2},{value:"Introduction",id:"introduction-1",level:3},{value:"Class Definition",id:"class-definition-1",level:3},{value:"Description",id:"description-1",level:4},{value:"Values",id:"values",level:4},{value:"Example",id:"example-1",level:3},{value:"<code>AlpacaEvalEvaluator</code>",id:"alpacaevalevaluator",level:2},{value:"Introduction",id:"introduction-2",level:3},{value:"Class Definition",id:"class-definition-2",level:3},{value:"Description",id:"description-2",level:4},{value:"Attributes",id:"attributes-1",level:4},{value:"Example",id:"example-2",level:3},{value:"Evaluating Sample Data with Alpaca Evaluator",id:"evaluating-sample-data-with-alpaca-evaluator",level:4},{value:"Use Alpaca Evaluator in the YiVal config",id:"use-alpaca-evaluator-in-the-yival-config",level:4},{value:"Source Code",id:"source-code-1",level:3},{value:"<code>BertScoreEvaluator</code>",id:"bertscoreevaluator",level:2},{value:"Introduction",id:"introduction-3",level:3},{value:"<code>BertScoreEvaluatorConfig</code>",id:"bertscoreevaluatorconfig",level:3},{value:"Description",id:"description-3",level:4},{value:"Attributes",id:"attributes-2",level:4},{value:"Example",id:"example-3",level:3},{value:"Source Code",id:"source-code-2",level:3},{value:"<code>OpenAIEloEvaluator</code>",id:"openaieloevaluator",level:2},{value:"Introduction",id:"introduction-4",level:3},{value:"Class Definition",id:"class-definition-3",level:3},{value:"Description",id:"description-4",level:4},{value:"Attributes",id:"attributes-3",level:4},{value:"Example",id:"example-4",level:3},{value:"Creating the <code>OpenAIEloEvaluator</code>Config File",id:"creating-the-openaieloevaluatorconfig-file",level:4},{value:"Use BertScoreEvaluator in YiVal Config",id:"use-bertscoreevaluator-in-yival-config",level:4},{value:"Source Code",id:"source-code-3",level:3},{value:"<code>PythonValidationEvaluator</code>",id:"pythonvalidationevaluator",level:2},{value:"Introduction",id:"introduction-5",level:3},{value:"Class Definition",id:"class-definition-4",level:3},{value:"Description",id:"description-5",level:4},{value:"Attributes",id:"attributes-4",level:4},{value:"Example",id:"example-5",level:3},{value:"Source Code",id:"source-code-4",level:3},{value:"<code>RougeEvaluator</code>",id:"rougeevaluator",level:2},{value:"Introduction",id:"introduction-6",level:3},{value:"Class Definition",id:"class-definition-5",level:3},{value:"Description",id:"description-6",level:4},{value:"Attributes",id:"attributes-5",level:4},{value:"Example",id:"example-6",level:3},{value:"Source Code",id:"source-code-5",level:3},{value:"<code>StringExpectedResultEvaluator</code>",id:"stringexpectedresultevaluator",level:2},{value:"Introduction",id:"introduction-7",level:3},{value:"Class Definition",id:"class-definition-6",level:3},{value:"Description",id:"description-7",level:4},{value:"Attributes",id:"attributes-6",level:4},{value:"Example",id:"example-7",level:3},{value:"UseStringExpectedResult in YiVal Config",id:"usestringexpectedresult-in-yival-config",level:4},{value:"Source Code",id:"source-code-6",level:3},{value:"Custom Evaluator Guide: <code>SimpleEvaluator</code>",id:"custom-evaluator-guide-simpleevaluator",level:2},{value:"Introduction",id:"introduction-8",level:3},{value:"Creating the SimpleEvaluator Configuration",id:"creating-the-simpleevaluator-configuration",level:3},{value:"Implementing the SimpleEvaluator",id:"implementing-the-simpleevaluator",level:3},{value:"Config",id:"config",level:3},{value:"Conclusion",id:"conclusion",level:3}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"evaluation",children:"Evaluation"}),"\n",(0,a.jsx)(n.h2,{id:"baseevaluator",children:(0,a.jsx)(n.code,{children:"BaseEvaluator"})}),"\n",(0,a.jsx)(n.h3,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(n.h3,{id:"class-definition",children:"Class Definition"}),"\n",(0,a.jsx)(n.h4,{id:"description",children:"Description"}),"\n",(0,a.jsx)(n.p,{children:"Evaluators are central components in the experimental framework that interpret experiment results and offer either quantitative or qualitative feedback."}),"\n",(0,a.jsx)(n.h4,{id:"attributes",children:"Attributes"}),"\n",(0,a.jsx)(n.h3,{id:"example",children:"Example"}),"\n",(0,a.jsx)(n.h3,{id:"source-code",children:(0,a.jsx)(n.a,{href:"https://github.com/YiVal/YiVal/blob/master/src/yival/evaluators/base_evaluator.py",children:"Source Code"})}),"\n",(0,a.jsx)(n.h2,{id:"evaluatortype",children:(0,a.jsx)(n.code,{children:"EvaluatorType"})}),"\n",(0,a.jsx)(n.h3,{id:"introduction-1",children:"Introduction"}),"\n",(0,a.jsx)(n.h3,{id:"class-definition-1",children:"Class Definition"}),"\n",(0,a.jsx)(n.h4,{id:"description-1",children:"Description"}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"EvaluatorType"})," enumeration class delineates the various evaluation methodologies available within the evaluator's module. Each type signifies a distinct approach to model assessment."]}),"\n",(0,a.jsx)(n.h4,{id:"values",children:"Values"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"INDIVIDUAL"})}),":","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"This evaluation type signifies that each model or output is evaluated on its own merit, without considering other variations or examples."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"COMPARISON"})}),":","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"In this evaluation type, one specific example is given, and models or outputs are compared based on how they handle that particular example."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"ALL"})}),":","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"This type indicates a comprehensive evaluation approach, where all available examples are considered. Each model or output is evaluated across the entirety of the provided dataset, giving a holistic view of its performance."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"example-1",children:"Example"}),"\n",(0,a.jsx)(n.h2,{id:"alpacaevalevaluator",children:(0,a.jsx)(n.code,{children:"AlpacaEvalEvaluator"})}),"\n",(0,a.jsx)(n.h3,{id:"introduction-2",children:"Introduction"}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"AlpacaEvalEvaluator"})," class is designed to facilitate evaluations using the Alpaca Eval system. This system is employed to rank different model outputs based on human evaluations. The specific implementation interfaces with the OpenAI API to carry out these evaluations, ensuring a blend of automation with human-like discernment."]}),"\n",(0,a.jsx)(n.h3,{id:"class-definition-2",children:"Class Definition"}),"\n",(0,a.jsx)(n.h4,{id:"description-2",children:"Description"}),"\n",(0,a.jsxs)(n.p,{children:["Tailored for the ",(0,a.jsx)(n.code,{children:"AlpacaEvalEvaluator"}),", this configuration class sets the guidelines and parameters for the evaluation process."]}),"\n",(0,a.jsx)(n.h4,{id:"attributes-1",children:"Attributes"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"alpaca_annotator_name(str)"})}),":","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Specifies the Alpaca Eval annotator to be used. The default value is ",(0,a.jsx)(n.code,{children:'"alpaca_eval_gpt4"'}),"."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"matching_technique(MatchingTechnique)"})}),":","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Determines the technique used for matching during the evaluation. The default value is ",(0,a.jsx)(n.code,{children:"MatchingTechnique.MATCH"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"example-2",children:"Example"}),"\n",(0,a.jsx)(n.h4,{id:"evaluating-sample-data-with-alpaca-evaluator",children:"Evaluating Sample Data with Alpaca Evaluator"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-Python",children:'# Creating a configuration for AlpacaEvalEvaluator\r\nevaluator_config = AlpacaEvalEvaluatorConfig(\r\n    name="alpaca_eval_evaluator",\r\n    alpaca_annotator_name="alpaca_eval_gpt4",\r\n    evaluator_type=EvaluatorType.COMPARISON\r\n)\r\n\r\n# Initializing the evaluator with the given configuration\r\nevaluator = AlpacaEvalEvaluator(evaluator_config)\r\n\r\n# Sample data for assessment\r\nsample_group_data = [...]  # A list of ExperimentResult objects\r\n\r\n# Evaluating the sample data\r\nevaluator.evaluate_comparison(sample_group_data)\r\n\r\n# Printing the evaluation results\r\nfor experiment in sample_group_data:\r\n    print(experiment.evaluator_outputs)\n'})}),"\n",(0,a.jsx)(n.h4,{id:"use-alpaca-evaluator-in-the-yival-config",children:"Use Alpaca Evaluator in the YiVal config"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-YAML",children:"evaluators:\r\n  - evaluator_type: comparison\r\n    alpaca_annotator_name: alpaca_eval_gpt4\r\n    metric_calculators:\r\n      - method: AVERAGE\r\n    name: alpaca_eval_evaluator\n"})}),"\n",(0,a.jsx)(n.h3,{id:"source-code-1",children:(0,a.jsx)(n.a,{href:"https://github.com/YiVal/YiVal/blob/master/src/yival/evaluators/alpaca_eval_evaluator.py",children:"Source Code"})}),"\n",(0,a.jsx)(n.h2,{id:"bertscoreevaluator",children:(0,a.jsx)(n.code,{children:"BertScoreEvaluator"})}),"\n",(0,a.jsx)(n.h3,{id:"introduction-3",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"BERTScore is an evaluation metric for language models based on the BERT language model. Instead of relying solely on token matching, BERTScore leverages the contextual embeddings from BERT to match words in the candidate and reference sentences using cosine similarity. This makes it robust against paraphrasing, allowing it to effectively gauge the semantic similarity between two sentences. Its strong correlation with human judgment, both on sentence-level and system-level evaluations, makes it an attractive metric for various NLP tasks."}),"\n",(0,a.jsx)(n.h3,{id:"bertscoreevaluatorconfig",children:(0,a.jsx)(n.code,{children:"BertScoreEvaluatorConfig"})}),"\n",(0,a.jsx)(n.h4,{id:"description-3",children:"Description"}),"\n",(0,a.jsxs)(n.p,{children:["Configuration class for the ",(0,a.jsx)(n.code,{children:"BertScoreEvaluator"}),". It specifies various parameters that dictate how the evaluation using BERTScore will be performed."]}),"\n",(0,a.jsx)(n.h4,{id:"attributes-2",children:"Attributes"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"evaluator_type"})}),":","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Specifies the type of evaluation. In the context of the ",(0,a.jsx)(n.code,{children:"BertScoreEvaluator"}),", it's set to ",(0,a.jsx)(n.code,{children:"EvaluatorType.INDIVIDUAL"}),", meaning each model output is evaluated individually against its corresponding expected result."]}),"\n",(0,a.jsxs)(n.li,{children:["The default value is ",(0,a.jsx)(n.code,{children:"EvaluatorType.INDIVIDUAL"}),"."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"description"})}),":","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"A descriptive text providing additional context or details about the evaluator."}),"\n",(0,a.jsxs)(n.li,{children:["The default value is  ",(0,a.jsx)(n.code,{children:'"This is the description of the evaluator"'}),"."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"lan"})}),":","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Language of the text being evaluated. BERTScore uses this to select the appropriate pre-trained BERT model. For instance, 'zh' corresponds to Chinese."}),"\n",(0,a.jsxs)(n.li,{children:["The default value is ",(0,a.jsx)(n.code,{children:"'zh'"}),"."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"indicator"})}),":","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Specifies which score to return out of precision (",(0,a.jsx)(n.code,{children:"p"}),"), recall (",(0,a.jsx)(n.code,{children:"r"}),"), or F1 score (",(0,a.jsx)(n.code,{children:"f"}),")."]}),"\n",(0,a.jsxs)(n.li,{children:["The default value is precision or ",(0,a.jsx)(n.code,{children:"'p'"}),"."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"name"})}),":","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Name of the evaluator."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"display_name"})}),":","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Display name for the metric."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"lan"})}),":","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Language of the text being evaluated. BERTScore uses this to select the appropriate pre-trained BERT model."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"metric_calculators"})}),":","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"List of additional metric calculators to be applied."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"example-3",children:"Example"}),"\n",(0,a.jsx)(n.p,{children:"In the usage example below, the evaluator is set up to compute the F1 BERTScore for a model's translation against the expected translation."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-Python",children:'evaluator_config = BertScoreEvaluatorConfig(\r\n    name="bertscore_evaluator",\r\n    display_name="bertscore",\r\n    lan="en",\r\n    indicator="f",\r\n    metric_calculators=[]\r\n)\r\n\r\ninput_data_example = InputData(\r\n    content={\r\n        "instruction": "Translate the sentence to English.",\r\n    },\r\n    expected_result="Have a great day!"\r\n)\r\n\r\nexperiment_result_example = ExperimentResult(\r\n    input_data=input_data_example,\r\n    combination={\r\n        "wrapper1": "var1",\r\n        "wrapper2": "var2"\r\n    },\r\n    raw_output=MultimodalOutput(text_output="Have a nice day!"),\r\n    latency=30.0,\r\n    token_usage=20\r\n)\r\n\r\nevaluator = BertScoreEvaluator(evaluator_config)\r\nresult = evaluator.evaluate(experiment_result_example)\r\nprint(result)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"source-code-2",children:(0,a.jsx)(n.a,{href:"https://github.com/YiVal/YiVal/blob/master/src/yival/evaluators/bertscore_evaluator.py",children:"Source Code"})}),"\n",(0,a.jsx)(n.h2,{id:"openaieloevaluator",children:(0,a.jsx)(n.code,{children:"OpenAIEloEvaluator"})}),"\n",(0,a.jsx)(n.h3,{id:"introduction-4",children:"Introduction"}),"\n",(0,a.jsxs)(n.p,{children:["This module provides an implementation of the ELO-based evaluation system in the ",(0,a.jsx)(n.code,{children:"OpenAIEloEvaluator"})," class. The ELO system ranks different model outputs based on human evaluations. This specific implementation interfaces with the OpenAI API to conduct those evaluations."]}),"\n",(0,a.jsx)(n.h3,{id:"class-definition-3",children:"Class Definition"}),"\n",(0,a.jsx)(n.h4,{id:"description-4",children:"Description"}),"\n",(0,a.jsxs)(n.p,{children:["Configuration class for the ",(0,a.jsx)(n.code,{children:"OpenAIEloEvaluator"}),". It specifies various parameters that dictate how the evaluation using the ELO system interfacing with OpenAI will be performed."]}),"\n",(0,a.jsx)(n.h4,{id:"attributes-3",children:"Attributes"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"openai_model_name"})}),":","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Specifies which OpenAI model to use for the evaluation."}),"\n",(0,a.jsxs)(n.li,{children:["The default is set to use the ",(0,a.jsx)(n.code,{children:'"gpt-4"'})," model."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"input_description"})}),":","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Provides a description of the input data that will be evaluated. This can be utilized when presenting data to the OpenAI model for ranking."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"example-4",children:"Example"}),"\n",(0,a.jsxs)(n.h4,{id:"creating-the-openaieloevaluatorconfig-file",children:["Creating the ",(0,a.jsx)(n.code,{children:"OpenAIEloEvaluator"}),"Config File"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-Python",children:'    evaluator = OpenAIEloEvaluator(\r\n        OpenAIEloEvaluatorConfig(\r\n            name="openai_elo_evaluator",\r\n            input_description="Translate the given English sentence to French",\r\n            evaluator_type=EvaluatorType.ALL,\r\n        )\r\n    )\r\n    experiment = create_test_data_v2()\r\n    evaluator.evaluate_based_on_all_results([experiment])\r\n    print(experiment)\n'})}),"\n",(0,a.jsx)(n.h4,{id:"use-bertscoreevaluator-in-yival-config",children:"Use BertScoreEvaluator in YiVal Config"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-YAML",children:"evaluators:\r\n  - evaluator_type: all\r\n    input_description:\r\n      Given an tech startup business, generate one corresponding landing\r\n      page headline\r\n    metric_calculators: []\r\n    name: openai_elo_evaluator\r\n    model_name: gpt-4\n"})}),"\n",(0,a.jsx)(n.h3,{id:"source-code-3",children:(0,a.jsx)(n.a,{href:"https://github.com/YiVal/YiVal/blob/master/src/yival/evaluators/openai_elo_evaluator.py",children:"Source Code"})}),"\n",(0,a.jsx)(n.h2,{id:"pythonvalidationevaluator",children:(0,a.jsx)(n.code,{children:"PythonValidationEvaluator"})}),"\n",(0,a.jsx)(n.h3,{id:"introduction-5",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"Evaluates the raw output of an experiment by attempting to execute it as Python code. If the code executes without any errors, a positive result is returned. Otherwise, a negative result is returned."}),"\n",(0,a.jsx)(n.h3,{id:"class-definition-4",children:"Class Definition"}),"\n",(0,a.jsx)(n.h4,{id:"description-5",children:"Description"}),"\n",(0,a.jsxs)(n.p,{children:["Configuration class for the ",(0,a.jsx)(n.code,{children:"PythonValidationEvaluator"}),". It specifies the matching technique to be used for the evaluation."]}),"\n",(0,a.jsx)(n.h4,{id:"attributes-4",children:"Attributes"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"matching_technique"})}),":","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["The default value is ",(0,a.jsx)(n.code,{children:"MatchingTechnique.MATCH"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"example-5",children:"Example"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-YAML",children:"evaluators:\r\n  - evaluator_type: individual\r\n    matching_technique: includes\r\n    metric_calculators:\r\n      - method: AVERAGE\r\n    name: python_validation_evaluator\n"})}),"\n",(0,a.jsx)(n.h3,{id:"source-code-4",children:(0,a.jsx)(n.a,{href:"https://github.com/YiVal/YiVal/blob/master/src/yival/evaluators/python_validation_evaluator.py",children:"Source Code"})}),"\n",(0,a.jsx)(n.h2,{id:"rougeevaluator",children:(0,a.jsx)(n.code,{children:"RougeEvaluator"})}),"\n",(0,a.jsx)(n.h3,{id:"introduction-6",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"The Rouge Evaluator is an advanced tool tailored for assessing the quality of dialogue model outputs. Leveraging the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metric, this evaluator emphasizes the relevance, coherence, and fluency of the generated content. Designed specifically for dialogue systems, the Rouge Evaluator offers developers and researchers a refined measure to gauge their model's efficiency, ensuring continuous improvement."}),"\n",(0,a.jsx)(n.h3,{id:"class-definition-5",children:"Class Definition"}),"\n",(0,a.jsx)(n.h4,{id:"description-6",children:"Description"}),"\n",(0,a.jsxs)(n.p,{children:["Configuration class for the ",(0,a.jsx)(n.code,{children:"RougeEvaluator"}),". It specifies the type of ROUGE metric to be used for the evaluation."]}),"\n",(0,a.jsx)(n.h4,{id:"attributes-5",children:"Attributes"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"evaluator_type"})}),":","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Specifies that the evaluator assesses each experiment individually."}),"\n",(0,a.jsxs)(n.li,{children:["The default value is ",(0,a.jsx)(n.code,{children:"EvaluatorType.INDIVIDUAL"}),"."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"description"})}),":","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"A brief description of the evaluator."}),"\n",(0,a.jsxs)(n.li,{children:["The default value is ",(0,a.jsx)(n.code,{children:'"This is the description of the evaluator"'}),"."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"rough_type"})}),":","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:'Specifies which type of ROUGE metric to use (e.g., "rouge-1", "rouge-2", "rouge-L").'}),"\n",(0,a.jsxs)(n.li,{children:["The default value is ",(0,a.jsx)(n.code,{children:'"rouge-1"'}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"example-6",children:"Example"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-Python",children:"import RougeEvaluator, RougeEvaluatorConfig\r\nimport ExperimentResult\r\n\r\nexperiment_result = ExperimentResult(\r\n    # ... your experiment result details here ...\r\n)\r\nconfig = RougeEvaluatorConfig()\r\nevaluator = RougeEvaluator(config)\r\noutput = evaluator.evaluate(experiment_result)\r\nprint(output)\n"})}),"\n",(0,a.jsx)(n.h3,{id:"source-code-5",children:(0,a.jsx)(n.a,{href:"https://github.com/YiVal/YiVal/blob/master/src/yival/evaluators/rouge_evaluator.py",children:"Source Code"})}),"\n",(0,a.jsx)(n.h2,{id:"stringexpectedresultevaluator",children:(0,a.jsx)(n.code,{children:"StringExpectedResultEvaluator"})}),"\n",(0,a.jsx)(n.h3,{id:"introduction-7",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"The String Expected Result Evaluator introduces a specialized class designed for evaluating expected results that are strings. This class extends the foundational evaluator and is equipped to compare actual and expected outputs using various matching techniques. The primary goal is to determine the accuracy of the generated strings in comparison to the expected outputs."}),"\n",(0,a.jsx)(n.h3,{id:"class-definition-6",children:"Class Definition"}),"\n",(0,a.jsx)(n.h4,{id:"description-7",children:"Description"}),"\n",(0,a.jsx)(n.p,{children:"This class extends the BaseEvaluator and provides specific implementation for evaluating string expected results using different matching techniques."}),"\n",(0,a.jsx)(n.h4,{id:"attributes-6",children:"Attributes"}),"\n",(0,a.jsx)(n.h3,{id:"example-7",children:"Example"}),"\n",(0,a.jsx)(n.h4,{id:"usestringexpectedresult-in-yival-config",children:"UseStringExpectedResult in YiVal Config"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-YAML",children:"evaluators:\r\n  - evaluator_type: individual\r\n    matching_technique: includes\r\n    metric_calculators:\r\n      - method: AVERAGE\r\n    name: string_expected_result\n"})}),"\n",(0,a.jsx)(n.h3,{id:"source-code-6",children:(0,a.jsx)(n.a,{href:"https://github.com/YiVal/YiVal/blob/master/src/yival/evaluators/string_expected_result_evaluator.py",children:"Source Code"})}),"\n",(0,a.jsxs)(n.h2,{id:"custom-evaluator-guide-simpleevaluator",children:["Custom Evaluator Guide: ",(0,a.jsx)(n.code,{children:"SimpleEvaluator"})]}),"\n",(0,a.jsx)(n.h3,{id:"introduction-8",children:"Introduction"}),"\n",(0,a.jsxs)(n.p,{children:["Evaluators are central components in the experimental framework that interpret experiment results and offer either quantitative or qualitative feedback. This guide will walk you through the steps of creating a custom evaluator, named ",(0,a.jsx)(n.code,{children:"SimpleEvaluator"}),", which returns a value of 1 if the result is 1, and 0 otherwise."]}),"\n",(0,a.jsx)(n.h3,{id:"creating-the-simpleevaluator-configuration",children:"Creating the SimpleEvaluator Configuration"}),"\n",(0,a.jsxs)(n.p,{children:["Before creating the evaluator, we define the configuration for our ",(0,a.jsx)(n.code,{children:"SimpleEvaluator"}),". The configuration helps in defining how the evaluator should behave and what parameters it may require."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-Python",children:'from dataclasses import dataclass, field\r\nfrom typing import Any, Dict, List\r\n\r\nfrom yival.schemas.evaluator_config import (\r\n    BaseEvaluatorConfig,\r\n    EvaluatorType,\r\n    MetricCalculatorConfig,\r\n)\r\n\r\n@dataclass\r\nclass SimpleEvaluatorConfig(BaseEvaluatorConfig):\r\n    """\r\n    Configuration for SimpleEvaluator.\r\n    """\r\n    metric_calculators: List[MetricCalculatorConfig] = field(\r\n        default_factory=list\r\n    )\r\n    evaluator_type = EvaluatorType.INDIVIDUAL\r\n\r\n    def asdict(self) -> Dict[str, Any]:\r\n        base_dict = super().asdict()\r\n        base_dict["metric_calculators"] = [\r\n            mc.asdict() for mc in self.metric_calculators\r\n        ]\r\n        return base_dict\n'})}),"\n",(0,a.jsx)(n.h3,{id:"implementing-the-simpleevaluator",children:"Implementing the SimpleEvaluator"}),"\n",(0,a.jsxs)(n.p,{children:["Now, let's create the ",(0,a.jsx)(n.code,{children:"SimpleEvaluator"})," that utilizes the above configuration:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-Python",children:'from simple_evaluator import SimpleEvaluatorConfig\r\nfrom yival.evaluators.base_evaluator import BaseEvaluator\r\nfrom yival.schemas.evaluator_config import EvaluatorOutput\r\n\r\n@BaseEvaluator.register("simple_evaluator")\r\nclass SimpleEvaluator(BaseEvaluator):\r\n    """\r\n    A basic evaluator that returns a value of 1 if the result is 1, and 0\r\n    otherwise.\r\n    """\r\n\r\n    def __init__(self, config: SimpleEvaluatorConfig):\r\n        super().__init__(config)\r\n\r\n    def evaluate(self, experiment_result) -> EvaluatorOutput:\r\n        """\r\n        Evaluate the experiment result and produce an evaluator output.\r\n\r\n        Args:\r\n            experiment_result: The result of an experiment to be evaluated.\r\n\r\n        Returns:\r\n            EvaluatorOutput: The result of the evaluation.\r\n        """\r\n        result = 1 if experiment_result == 1 else 0\r\n        return EvaluatorOutput(name="Simple Evaluation", result=result)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"config",children:"Config"}),"\n",(0,a.jsx)(n.p,{children:"Next you can config the evaluator"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-YAML",children:"custom_evaluators:\r\n  simple_evaluator:\r\n    class: /path/to/simple_evaluator.SimpleEvaluator\r\n    config_cls: /path/to/simple_evaluator_config.SimpleEvaluatorConfig\n"})}),"\n",(0,a.jsx)(n.p,{children:"And use it"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-YAML",children:"evaluators:\r\n  - name: simple_evaluator\r\n    simple_evaluator:\r\n      metric_calculators: []\n"})}),"\n",(0,a.jsx)(n.h3,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsxs)(n.p,{children:["By following this guide, you've successfully developed, configured, and registered  a custom evaluator named ",(0,a.jsx)(n.code,{children:"SimpleEvaluator"})," within the experimental framework.  Custom evaluators, like the one you've created, enable a tailored approach to interpreting and analyzing experiment results, ensuring the specific needs of an experiment are met."]})]})}function u(e={}){const{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},1151:(e,n,i)=>{i.d(n,{Z:()=>o,a:()=>r});var a=i(7294);const t={},l=a.createContext(t);function r(e){const n=a.useContext(l);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),a.createElement(l.Provider,{value:n},e.children)}}}]);